{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
        "\n",
        "**DeepSeek-R1** is a state-of-the-art reasoning model combining reinforcement learning and supervised fine-tuning, excelling at complex reasoning tasks with 37B active parameters and 128K context window.\n",
        "\n",
        "In this notebook, you'll learn to:\n",
        "1. **Initialize** the ChatCompletionsClient for Azure serverless endpoints\n",
        "2. **Chat** with DeepSeek-R1 using reasoning extraction\n",
        "3. **Implement** a travel planning example with step-by-step reasoning\n",
        "4. **Leverage** the 128K context window for complex scenarios\n",
        "\n",
        "## Why DeepSeek-R1?\n",
        "- **Advanced Reasoning**: Specializes in chain-of-thought problem solving\n",
        "- **Massive Context**: 128K token window for detailed analysis\n",
        "- **Efficient Architecture**: 37B active parameters from 671B total\n",
        "- **Safety Integrated**: Built-in content filtering capabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication\n",
        "\n",
        "Required packages:\n",
        "- `azure-ai-inference`: For chat completions\n",
        "- `python-dotenv`: For environment variables\n",
        "\n",
        ".env file requirements:\n",
        "```bash\n",
        "AZURE_INFERENCE_ENDPOINT=<your-endpoint-url>\n",
        "AZURE_INFERENCE_KEY=<your-api-key>\n",
        "MODEL_NAME=DeepSeek-R1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found cred.json at: d:\\MLOps\\Gen Ai & MLOps Masterclass\\Materilas\\test\\ai-foundry-workshop\\cred.json\n",
            "Azure Inference Endpoint: https://ai-sarath5717ai038608807015.services.ai.azure.com/models\n",
            "Azure Inference Key: ****UVBn\n",
            "DeepSeek-R1: DeepSeek-R1\n",
            "Model name: DeepSeek-R1\n",
            "‚úÖ Client initialized | Model: deepseek-r1\n",
            "\n",
            "Test response: <think>\n",
            "Okay, the user greeted me with \"Hello! How are you?\" I need to respond appropriately. Since I'm an AI, I don't have feelings, but I should acknowledge their greeting and ask how I can assist them. Keep it friendly and open-ended. Make sure to invite them to ask for help with anything they need. Maybe mention that I'm here to help with questions, information, or just chatting. Keep the tone positive and welcoming.\n",
            "</think>\n",
            "\n",
            "Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need! How can I assist you today? Whether it's answering questions, brainstorming ideas, or just chatting, I'm all ears. üòä\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "def find_cred_json(start_path):\n",
        "    # Start from current directory and go up\n",
        "    current = Path(start_path)\n",
        "    while current != current.parent:  # while we haven't hit the root\n",
        "        cred_file = current / 'cred.json'\n",
        "        if cred_file.exists():\n",
        "            return str(cred_file)\n",
        "        current = current.parent\n",
        "    return None\n",
        "\n",
        "try:\n",
        "    # Search in the parent directory and its subdirectories\n",
        "    parent_dir = os.path.dirname(os.getcwd())  # Get parent directory\n",
        "    file_path = find_cred_json(parent_dir)\n",
        "\n",
        "    if not file_path:\n",
        "        raise FileNotFoundError(\"cred.json not found in parent directories\")\n",
        "\n",
        "    print(f\"Found cred.json at: {file_path}\")\n",
        "\n",
        "    # Load and parse the JSON file\n",
        "    with open(file_path, 'r') as f:\n",
        "        loaded_config = json.load(f)\n",
        "        \n",
        "    print(\"Azure Inference Endpoint:\", loaded_config['AZURE_INFERENCE_ENDPOINT'])\n",
        "    print(\"Azure Inference Key:\", \"****\" + loaded_config['AZURE_INFERENCE_KEY'][-4:])  # Print last 4 chars only for security\n",
        "    print(\"DeepSeek-R1:\", loaded_config['MODEL_NAME'])\n",
        "\n",
        "    endpoint = loaded_config.get(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "    key = loaded_config.get(\"AZURE_INFERENCE_KEY\")\n",
        "    model_name = loaded_config.get(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "    print(\"Model name:\", model_name)\n",
        "\n",
        "    # Initialize client with model name in headers\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key),\n",
        "        headers={\n",
        "            \"x-ms-model-mesh-model-name\": model_name\n",
        "        }\n",
        "    )\n",
        "    print(\"‚úÖ Client initialized | Model:\", client.get_model_info().model_name)\n",
        "\n",
        "    # Test the client with a simple message\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "        UserMessage(content=\"Hello! How are you?\")\n",
        "    ]\n",
        "    \n",
        "    response = client.complete(\n",
        "        model=model_name,\n",
        "        messages=messages\n",
        "    )\n",
        "    \n",
        "    # Extract the response content\n",
        "    if response and hasattr(response, 'choices') and response.choices:\n",
        "        assistant_message = response.choices[0].message.content\n",
        "        print(\"\\nTest response:\", assistant_message)\n",
        "    else:\n",
        "        print(\"\\n‚ùå No response content received\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Could not find file: {str(e)}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"‚ùå File exists but contains invalid JSON: {str(e)}\")\n",
        "except KeyError as e:\n",
        "    print(f\"‚ùå Missing required key in config file: {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Unexpected error: {str(e)}\")\n",
        "\n",
        "# Optional: Add an interactive chat loop\n",
        "def chat_loop(client, model_name):\n",
        "    conversation = [\n",
        "        SystemMessage(content=\"You are a helpful assistant.\")\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nChat started (type 'exit' to end)\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "            \n",
        "        conversation.append(UserMessage(content=user_input))\n",
        "        \n",
        "        try:\n",
        "            response = client.complete(\n",
        "                model=model_name,\n",
        "                messages=conversation\n",
        "            )\n",
        "            \n",
        "            if response and hasattr(response, 'choices') and response.choices:\n",
        "                assistant_message = response.choices[0].message.content\n",
        "                print(f\"Assistant: {assistant_message}\")\n",
        "                conversation.append(SystemMessage(content=assistant_message))\n",
        "            else:\n",
        "                print(\"‚ùå No response received\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "# Uncomment to enable interactive chat\n",
        "# if 'client' in locals():\n",
        "#     chat_loop(client, model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. Intelligent Travel Planning ‚úàÔ∏è\n",
        "\n",
        "Demonstrate DeepSeek-R1's reasoning capabilities for trip planning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üó∫Ô∏è Query: Plan a 5-day cultural trip to Kyoto in April\n",
            "\n",
            "üìù Response: <think>\n",
            "Okay, I need to plan a 5-day cultural trip to Kyoto in April. Let me start by recalling what I know about Kyoto. It's a city rich in history, temples, gardens, and traditional culture. April is cherry blossom season, so that's a big plus. The user wants hidden gems and safety considerations. \n",
            "\n",
            "First, I should outline the structure: 5 days, each day with a theme or area. Must include main attractions but also lesser-known spots. Safety tips for each day. Starting with arrival day, maybe Day 1 is arrival and central Kyoto. Then each subsequent day covers different areas.\n",
            "\n",
            "Hidden gems: places not overrun by tourists. Maybe places like smaller temples, local neighborhoods, less crowded spots even during cherry blossom season. Examples I know: Tofuku-ji's hidden gardens, Shisen-do, Ohara area. Also, the Philosopher's Path might be crowded, but maybe suggest early morning visits. \n",
            "\n",
            "Safety considerations: April weather is mild but possible rain. Slippery paths in temples, crowded areas pickpocketing (though rare in Japan, still a consideration). Also, COVID-19 precautions if any. Language barriers, emergency numbers. Maybe advise on comfortable footwear, carrying cash since some places don't take cards.\n",
            "\n",
            "Day 1: Arrival and Central Kyoto. Kiyomizu-dera is a must, but maybe add nearby Ninenzaka and Sannenzaka streets. Hidden gem: Ishibei-koji Lane. Evening in Gion. Safety: crowded streets, watch steps on old pavements.\n",
            "\n",
            "Day 2: Arashiyama area. Bamboo Grove early to avoid crowds. Then Tenryu-ji. Hidden gem: Otagi Nenbutsu-ji Temple with the quirky statues. Monkey Park Iwatayama. Safety: monkeys can be aggressive, so follow guidelines. Also, trail might be slippery.\n",
            "\n",
            "Day 3: Northern Kyoto. Maybe Ryoan-ji and Kinkaku-ji (Golden Pavilion), but those can be busy. Hidden gem: Shisen-do Temple in the east. Then Ohara village, Sanzen-in Temple. Safety: rural areas might have less English, transportation schedules.\n",
            "\n",
            "Day 4: Eastern Kyoto. Philosopher's Path starting early, Nanzen-ji, then Ginkaku-ji. Hidden gem: Honen-in Temple. Safety: cycling paths, bike safety. Maybe mention renting bikes.\n",
            "\n",
            "Day 5: Day trip to Uji. Byodo-in Temple, Ujigami Shrine. Tea experiences since Uji is famous for matcha. Hidden gem: Mimuroto-ji Temple with hydrangeas, but in April maybe other flowers. Safety: train schedules, checking last train times.\n",
            "\n",
            "Also, general safety tips: emergency numbers, carrying hotel address, JR Pass usage, cash. Cherry blossoms mean hanami spots can be crowded; suggest alternatives.\n",
            "\n",
            "Wait, need to check if some temples have entry fees, opening hours. Also, transportation between areas. Maybe suggest using buses or trains, but Kyoto's buses can be crowded. Maybe recommend IC cards like Suica or Pasmo.\n",
            "\n",
            "For each day, structure: Morning, midday, afternoon, evening. Include rationale why each spot is chosen, how it's a hidden gem. Safety tips specific to activities. Maybe mention local dining spots for lunch, like in Nishiki Market on Day 1.\n",
            "\n",
            "Need to make sure the plan is not too packed. Balance between sightseeing and cultural experiences, like tea ceremonies, kimono rentals. Also, April weather: average temps around 15-20¬∞C, possible rain, so recommend layers and umbrella.\n",
            "\n",
            "Hidden gems: Maybe Enko-ji Temple in northern Kyoto for autumn, but April? Maybe Daigo-ji Temple's gardens, which are a bit further out. Or the Fushimi Sake District if including a sake brewery visit. But user asked for cultural, so maybe stick to temples and traditional areas.\n",
            "\n",
            "Wait, Ohara is a good hidden gem. Sanzen-in is beautiful and maybe less crowded. Also, the Kurama area, but that might be a bit far for a day trip. Need to check locations to make sure travel time is manageable.\n",
            "\n",
            "Day 3: Maybe combine Ryoan-ji and Kinkaku-ji in the morning, then head to Ohara in the afternoon. Sanzen-in in Ohara is a good hidden gem. Transportation: bus from Kyoto to Ohara, takes about an hour.\n",
            "\n",
            "For dining, recommend trying kaiseki or yudofu (tofu hot pot) in Ohara. Safety considerations: buses might be infrequent, so check schedules.\n",
            "\n",
            "Day 4: Philosopher's Path. Early morning to avoid crowds. Ginkaku-ji (Silver Pavilion) is nearby. Honen-in is quieter. Lunch in the area, maybe near Nanzen-ji. Then maybe a tea ceremony experience in the afternoon.\n",
            "\n",
            "Safety for cycling: road rules, where to park bikes.\n",
            "\n",
            "Day 5:\n"
          ]
        }
      ],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a travel expert. Provide detailed plans with rationale.\"),\n",
        "        UserMessage(content=f\"{query} Include hidden gems and safety considerations.\")\n",
        "    ]\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=messages,\n",
        "        model=model_name,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract reasoning if present\n",
        "    if show_thinking:\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "    return content\n",
        "\n",
        "# Example usage\n",
        "query = \"Plan a 5-day cultural trip to Kyoto in April\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"üó∫Ô∏è Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nüìù Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. Technical Problem Solving üíª\n",
        "\n",
        "Showcase coding/optimization capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Problem: How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
            "Consider indexing strategies, hardware requirements, and query optimization.\n",
            "\n",
            "‚öôÔ∏è Solution: <think>\n",
            "Okay, so I need to figure out how to optimize a PostgreSQL database that's handling 10k transactions per second. That's a pretty high load, so I need to make sure everything is tuned properly. Let me start by breaking down the problem into parts. The user mentioned indexing strategies, hardware requirements, and query optimization. I'll tackle each of these areas one by one, but I know they're all interconnected.\n",
            "\n",
            "First, indexing. I remember that indexes can speed up read queries, but they can also slow down writes because every insert, update, or delete has to update the index. So for a high transaction rate, having the right indexes is crucial. But I shouldn't over-index. Maybe start by looking at the query patterns. If there are frequent queries on certain columns, those should be indexed. Composite indexes might be useful if queries filter on multiple columns. Also, maybe using indexes like BRIN for time-series data if applicable, since they're smaller and faster for ranges. But wait, BRIN is good for large tables with naturally ordered data, like timestamps. If the data isn't ordered, maybe not. Also, partial indexes could help if queries often select a subset of data, like active users. That way, the index is smaller and more efficient. Index bloat could be an issue too, so regular maintenance like REINDEX or using CONCURRENTLY to rebuild indexes without locking might be necessary. Oh, and maybe covering indexes (INCLUDE) to allow index-only scans, so the query doesn't have to hit the table.\n",
            "\n",
            "Next, hardware. For 10k TPS, the hardware needs to handle both CPU and I/O. Postgres is usually more disk I/O bound, especially with write-heavy workloads. So SSDs are a must. NVMe drives would be better for lower latency. The CPU should have high clock speed and multiple cores, but Postgres is somewhat single-threaded per connection, so maybe having enough cores to handle parallel queries and connections. Memory is critical. The shared_buffers setting in Postgres should be adjusted to utilize available RAM effectively, maybe around 25% of total RAM, but not more than that. Also, the OS cache is important, so having enough RAM to keep the working set in memory. If the database is larger than RAM, that could cause swapping, which is bad. Maybe using a RAID configuration for disks, or considering storage solutions with high IOPS. Network bandwidth might also be a factor if there's replication or if the application servers are separate from the database.\n",
            "\n",
            "Query optimization. Even with good indexes and hardware, bad queries can kill performance. Need to analyze slow queries using EXPLAIN ANALYZE or auto_explain module. Look for sequential scans on large tables, which indicate missing indexes. Also, check for query plans that aren't using indexes effectively, maybe due to data type mismatches or functions in WHERE clauses that prevent index usage. Joins without proper indexes on joined columns can be slow. Maybe rewriting queries to be more efficient, using joins instead of subqueries where possible. Reducing the number of round trips by batching queries or using prepared statements. Connection pooling is important too, because opening a new connection for each transaction is expensive. Using something like PgBouncer to manage connections. Also, tuning Postgres configuration parameters like work_mem, maintenance_work_mem, max_connections, and checkpoint settings. For example, increasing max_wal_size to reduce the frequency of checkpoints, which can cause I/O spikes. Autovacuum settings need to be optimized to handle MVCC cleanup, especially with high write rates. If autovacuum isn't keeping up, dead tuples can accumulate and slow down queries.\n",
            "\n",
            "Partitioning might help. If the tables are very large, partitioning by range or list can make indexes smaller and queries faster by pruning partitions. Sharding is another option, but that's more complex and might require application changes. Replication for read scaling. Using read replicas to offload read queries, allowing the primary to handle writes. But for 10k TPS, maybe the writes are the main concern. Maybe considering connection pooling to handle the high number of concurrent connections without overloading the database.\n",
            "\n",
            "Wait, also consider locking issues. High concurrency can lead to lock contention. Using row-level locks appropriately, maybe avoiding long-running transactions. Using the right transaction isolation levels. Maybe using optimistic locking if applicable. Monitoring for deadlocks and long-running queries.\n",
            "\n",
            "What about the schema design? Normalization vs. denormalization. If the schema is overly normalized, joins might be slowing things down. But denormalization can increase write overhead. Need to balance based on the specific workload.\n",
            "\n",
            "For hardware, maybe scaling vertically first (bigger machine with more cores, RAM, fast storage) and then horizontally if needed. But Postgres scaling horizontally is tricky. Maybe using Citus for sharding, but that's an extension.\n",
            "\n",
            "Also, consider using connection poolers like PgBouncer in transaction mode to reduce the number of actual Postgres connections. Each connection in Postgres is a process, and too many can cause overhead.\n",
            "\n",
            "Logging and monitoring. Tools like pg_stat_statements to identify slow queries. Setting up alerts for long transactions, lock waits, etc. Using tools like Prometheus and Grafana for monitoring performance metrics.\n",
            "\n",
            "Tuning the vacuum settings. With high write volume, autovacuum needs to be aggressive enough to prevent transaction ID wraparound and keep tables clean. Adjusting autovacuum_vacuum_scale_factor and autovacuum_vacuum_threshold for heavily updated tables.\n",
            "\n",
            "Checkpoint tuning. Making sure that checkpoints are spaced out enough to avoid I/O spikes. Adjusting checkpoint_completion_target to 0.9 to spread writes over time.\n",
            "\n",
            "WAL configuration. Putting WAL on a separate disk to avoid contention with data disks. Tuning wal_buffers and wal_writer_delay.\n",
            "\n",
            "Parallel query settings. If queries can benefit from parallel execution, adjusting max_parallel_workers and related parameters.\n",
            "\n",
            "In summary, the main areas are:\n",
            "\n",
            "1. Proper indexing strategies tailored to the workload.\n",
            "2. Adequate hardware with fast storage, sufficient RAM, and CPU.\n",
            "3. Optimized queries and schema design.\n",
            "4. Tuning Postgres configuration parameters.\n",
            "5. Connection pooling and managing connections.\n",
            "6. Regular maintenance (vacuum, analyze, reindex).\n",
            "7. Monitoring and continuous analysis.\n",
            "\n",
            "I need to make sure all these aspects are covered. Maybe start with analyzing the current bottlenecks. Without knowing the exact workload, it's hard to be specific, but these are general best practices for high TPS in Postgres.\n",
            "</think>\n",
            "\n",
            "To optimize a PostgreSQL database handling **10k transactions/second**, address the following areas systematically:\n",
            "\n",
            "### **1. Indexing Strategies**\n",
            "- **Targeted Indexes**: Create indexes on frequently queried columns and composite indexes for multi-column filters. Avoid over-indexing to reduce write overhead.\n",
            "- **Partial Indexes**: Use for queries filtering on a subset (e.g., `WHERE status = 'active'`).\n",
            "- **BRIN Indexes**: For large time-series tables with ordered data (e.g., timestamps).\n",
            "- **Covering Indexes**: Include frequently accessed columns with `INCLUDE` to enable index-only scans.\n",
            "- **Maintenance**: Rebuild bloated indexes concurrently (`REINDEX CONCURRENTLY`) and monitor index usage with `pg_stat_user_indexes`.\n",
            "\n",
            "### **2. Hardware Requirements**\n",
            "- **Storage**: Use NVMe SSDs for low-latency I/O. Separate WAL and data disks to avoid contention.\n",
            "- **RAM**: Ensure sufficient memory to hold the working dataset. Configure `shared_buffers` (25% of total RAM) and let the OS cache handle the rest.\n",
            "- **CPU**: High-clock-speed, multi-core processors for parallel query execution.\n",
            "- **Network**: Low-latency, high-bandwidth connections for distributed systems or replication.\n",
            "\n",
            "### **3. Query Optimization**\n",
            "- **Analyze Queries**: Use `EXPLAIN ANALYZE` and `pg_stat_statements` to identify slow queries.\n",
            "- **Avoid Sequential Scans**: Ensure WHERE clauses use indexed columns. Fix data type mismatches and avoid functions that disable indexes.\n",
            "- **Batch Operations**: Reduce round trips with batched writes/reads and prepared statements.\n",
            "- **Normalization/Denormalization**: Balance schema design to minimize joins or redundant writes.\n",
            "\n",
            "### **4. PostgreSQL Configuration**\n",
            "- **Memory Settings**: Adjust `work_mem` (for sorting/joins) and `maintenance_work_mem` (for vacuuming).\n",
            "- **Checkpoints**: Increase `max_wal_size` and set `checkpoint_completion_target = 0.9` to smooth I/O spikes.\n",
            "- **Autovacuum**: Tune `autovacuum_vacuum_scale_factor` and `autovacuum_vacuum_threshold` for high-churn tables.\n",
            "- **Parallelism**: Enable `max_parallel_workers` for CPU-heavy analytic queries.\n",
            "\n",
            "### **5. Connection Management**\n",
            "- **Pooling**: Use PgBouncer or pgPool-II to limit active connections and reuse sessions.\n",
            "- **Idle Timeouts**: Set `idle_in_transaction_session_timeout` to kill stuck transactions.\n",
            "\n",
            "### **6. Scalability & Maintenance**\n",
            "- **Partitioning**: Split large tables by range/list (e.g., by time) to reduce index size and improve pruning.\n",
            "- **Replication**: Offload reads to replicas. Consider synchronous replication for critical data.\n",
            "- **Monitoring**: Use Prometheus/Grafana for metrics (CPU, I/O, locks) and alerting.\n",
            "\n",
            "### **7. Advanced Tactics**\n",
            "- **Write-Ahead Log (WAL) Tuning**: Optimize `wal_buffers` and `wal_writer_delay`.\n",
            "- **Lock Contention**: Monitor with `pg_locks` and use row-level locks or optimistic con\n"
          ]
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            UserMessage(content=f\"{problem} Please reason step by step, and put your final answer within \\boxed{{}}.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
        "Consider indexing strategies, hardware requirements, and query optimization.\"\"\"\n",
        "\n",
        "print(\"üîß Problem:\", problem)\n",
        "print(\"\\n‚öôÔ∏è Solution:\", solve_technical_problem(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9f7a8c",
      "metadata": {},
      "source": [
        "## 4. Best Practices & Considerations\n",
        "\n",
        "1. **Reasoning Handling**: Use regex to separate <think> content from final answers\n",
        "2. **Safety**: Built-in content filtering - handle HttpResponseError for violations\n",
        "3. **Performance**:\n",
        "   - Max tokens: 4096\n",
        "   - Rate limit: 200K tokens/minute\n",
        "4. **Cost**: Pay-as-you-go with serverless deployment\n",
        "5. **Streaming**: Implement response streaming for long completions\n",
        "\n",
        "```python\n",
        "# Streaming example\n",
        "response = client.complete(..., stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "- Leverage 128K context for detailed analysis\n",
        "- Extract reasoning steps for debugging/analysis\n",
        "- Combine with Azure AI Content Safety for production\n",
        "- Monitor token usage via response.usage\n",
        "\n",
        "> Always validate model outputs for critical applications!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
