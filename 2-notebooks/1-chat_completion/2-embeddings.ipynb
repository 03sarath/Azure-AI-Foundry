{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3e8f66",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Fun with Text and Image Embeddings üçé\n",
    "\n",
    "Welcome to our **Health & Fitness** embeddings notebook! In this tutorial, we'll show you how to:\n",
    "\n",
    "1. **Initialize** an `AIProjectClient` to access your Azure AI Foundry project.\n",
    "2. **Embed text** using `azure-ai-inference` with our fun health-themed phrases.\n",
    "3. **Embed images** using `azure-ai-inference` (we're using Cohere's image embeddings model).\n",
    "4. **Generate a health-themed image** (example code) and display it.\n",
    "5. **Use a prompt template** for extra context.\n",
    "\n",
    "Let's get started and have some fun with our healthy ideas! üçè\n",
    "\n",
    "> **Disclaimer**: This notebook is for educational purposes only. Always consult a professional for medical advice.\n",
    "\n",
    "<img src=\"./seq-diagrams/2-embeddings.png\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd2ac6",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment\n",
    "\n",
    "#### Prerequisites:\n",
    "- Deploy a text embeddings model (**text-embedding-3-small**) in Azure AI Foundry\n",
    "- Deploy a image embeddings model (**Cohere-embed-v3-english**) in Azure AI Foundry\n",
    "#\n",
    "We'll import our libraries and load the environment variables for:\n",
    "- `PROJECT_CONNECTION_STRING`: Your project connection string.\n",
    "- `TEXT_EMBEDDING_MODEL`: The text embeddings model deployment name.\n",
    "- `IMAGE_EMBEDDING_MODEL`: The image embeddings model deployment name.\n",
    "\n",
    "We'll import libraries, load environment variables, and create an `AIProjectClient`.\n",
    "\n",
    "> #### Complete [1-basic-chat-completion.ipynb](./1-basic-chat-completion.ipynb) notebook before starting this one\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccfda179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cred.json at: d:\\MLOps\\Gen Ai & MLOps Masterclass\\Materilas\\test\\ai-foundry-workshop\\cred.json\n",
      "Project Connection String: eastus2.api.azureml.ms;1c2fd79b-ad21-4ad0-8d53-12de16650452;rg-sarath-4339_ai;sarath-1143\n",
      "Tenant ID: 02e58275-def8-41c4-82c4-f7864c28f7c9\n",
      "Model Deployment ID: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def find_cred_json(start_path):\n",
    "    # Start from current directory and go up\n",
    "    current = Path(start_path)\n",
    "    while current != current.parent:  # while we haven't hit the root\n",
    "        cred_file = current / 'cred.json'\n",
    "        if cred_file.exists():\n",
    "            return str(cred_file)\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "# Search in the parent directory and its subdirectories\n",
    "parent_dir = os.path.dirname(os.getcwd())  # Get parent directory\n",
    "file_path = find_cred_json( parent_dir)\n",
    "\n",
    "print(f\"Found cred.json at: {file_path}\")\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as f:\n",
    "        loaded_config = json.load(f)\n",
    "    \n",
    "    # Print the values to verify\n",
    "    print(\"Project Connection String:\", loaded_config['PROJECT_CONNECTION_STRING'])\n",
    "    print(\"Tenant ID:\", loaded_config['TENANT_ID'])\n",
    "    print(\"Model Deployment ID:\", loaded_config['MODEL_DEPLOYMENT_NAME'])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not find file at: {file_path}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"File exists but contains invalid JSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50899c96",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Successfully created AIProjectClient\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference.models import ImageEmbeddingInput\n",
    "\n",
    "# Retrieve from environment or fallback\n",
    "PROJECT_CONNECTION_STRING = loaded_config.get(\"PROJECT_CONNECTION_STRING\", \"<your-connection-string>\")\n",
    "TEXT_EMBEDDING_MODEL = loaded_config.get(\"TEXT_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "IMAGE_EMBEDDING_MODEL = loaded_config.get(\"IMAGE_EMBEDDING_MODEL\", \"Cohere-embed-v3-english\")\n",
    "\n",
    "# Initialize project client\n",
    "try:\n",
    "    project_client = AIProjectClient.from_connection_string(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        conn_str=PROJECT_CONNECTION_STRING,\n",
    "    )\n",
    "    print(\"üéâ Successfully created AIProjectClient\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error creating AIProjectClient:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970dcce",
   "metadata": {},
   "source": [
    "## 2. Text Embeddings\n",
    "\n",
    "We'll call `get_embeddings_client()` from our `AIProjectClient` to retrieve the embeddings client. Then we'll embed some fun health-themed phrases:\n",
    "\n",
    "- \"üçé An apple a day keeps the doctor away\"\n",
    "- \"üèãÔ∏è 15-minute HIIT workout routine\"\n",
    "- \"üßò Mindful breathing exercises\"\n",
    "\n",
    "The output will be numeric vectors representing each phrase in semantic space. Let‚Äôs see those embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28466a95",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0: 'An apple a day keeps the doctor away üçé':\n",
      "  Embedding length=1536\n",
      "  Sample: [-0.0213, -0.0037, ..., -0.0084, 0.0337]\n",
      "\n",
      "Sentence 1: 'Quick 15-minute HIIT workout routine üèãÔ∏è':\n",
      "  Embedding length=1536\n",
      "  Sample: [-0.0583, 0.0209, ..., 0.0095, -0.0158]\n",
      "\n",
      "Sentence 2: 'Mindful breathing exercises üßò':\n",
      "  Embedding length=1536\n",
      "  Sample: [-0.0097, 0.0470, ..., 0.0402, 0.0018]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_phrases = [\n",
    "    \"An apple a day keeps the doctor away üçé\",\n",
    "    \"Quick 15-minute HIIT workout routine üèãÔ∏è\",\n",
    "    \"Mindful breathing exercises üßò\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    with project_client.inference.get_embeddings_client() as embed_client:\n",
    "        response = embed_client.embed(\n",
    "            model=TEXT_EMBEDDING_MODEL,\n",
    "            input=text_phrases\n",
    "        )\n",
    "\n",
    "        for item in response.data:\n",
    "            vec = item.embedding\n",
    "            sample_str = f\"[{vec[0]:.4f}, {vec[1]:.4f}, ..., {vec[-2]:.4f}, {vec[-1]:.4f}]\"\n",
    "            print(f\"Sentence {item.index}: '{text_phrases[item.index]}':\\n\" \\\n",
    "                  f\"  Embedding length={len(vec)}\\n\" \\\n",
    "                  f\"  Sample: {sample_str}\\n\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error embedding text:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b0402",
   "metadata": {},
   "source": [
    "## 3. Prompt Template Example üìù\n",
    "\n",
    "Even though our focus is on embeddings, here's how you might prepend some context to a user message. Imagine you want to embed user text but first add a system prompt such as ‚ÄúYou are HealthFitGPT, a fitness guidance model‚Ä¶‚Äù This little extra helps set the stage for more context-aware embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f3e392",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "First few dims: [0.00024832287, 0.038954947, -0.01699466, 0.025093678, -0.040707525, -0.0024197476, -0.03595433, 0.07275839]\n"
     ]
    }
   ],
   "source": [
    "# A basic prompt template (system-style) we'll prepend to user text.\n",
    "TEMPLATE_SYSTEM = (\n",
    "    \"You are HealthFitGPT, a fitness guidance model.\\n\"\n",
    "    \"Please focus on healthy advice and disclaim you're not a doctor.\\n\\n\"\n",
    "    \"User message:\"  # We'll append the user message after this.\n",
    ")\n",
    "\n",
    "def embed_with_template(user_text):\n",
    "    \"\"\"Embed user text with a system template in front.\"\"\"\n",
    "    content = TEMPLATE_SYSTEM + \" \" + user_text\n",
    "    with project_client.inference.get_embeddings_client() as embed_client:\n",
    "        rsp = embed_client.embed(\n",
    "            model=TEXT_EMBEDDING_MODEL,\n",
    "            input=[content]\n",
    "        )\n",
    "    return rsp.data[0].embedding\n",
    "\n",
    "sample_user_text = \"Can you suggest a quick home workout for busy moms?\"\n",
    "embedding_result = embed_with_template(sample_user_text)\n",
    "print(\"Embedding length:\", len(embedding_result))\n",
    "print(\"First few dims:\", embedding_result[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571c972",
   "metadata": {},
   "source": [
    "## 4. Image Embeddings\n",
    "\n",
    "Image embeddings typically require managed compute resources. While Azure AI Foundry offers specialized models (like **MedImageInsight**) for medical images, in this example we'll use Cohere's serverless image embedding model.\n",
    "\n",
    "Here we are using the **`hand-xray.png`** image to generate embeddings. This image (of a hand X-ray) is our fun nod to health-themed imagery!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "353bbc40",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image index=0, length=1024, sample=[0.0314, -0.0194, ..., -0.0091, 0.0015]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with project_client.inference.get_image_embeddings_client() as img_embed_client:\n",
    "        # Construct input for the image embeddings call\n",
    "        img_input = ImageEmbeddingInput.load(image_file=\"hand-xray.png\", image_format=\"png\")\n",
    "        resp = img_embed_client.embed(\n",
    "            model=IMAGE_EMBEDDING_MODEL,\n",
    "            input=[img_input]\n",
    "        )\n",
    "\n",
    "        for item in resp.data:\n",
    "            vec = item.embedding\n",
    "            snippet = f\"[{vec[0]:.4f}, {vec[1]:.4f}, ..., {vec[-2]:.4f}, {vec[-1]:.4f}]\"\n",
    "            print(f\"Image index={item.index}, length={len(vec)}, sample={snippet}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error embedding image:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb863ae9",
   "metadata": {},
   "source": [
    "## 5. Generate a Health-Related Image üèÉ (Optional)\n",
    "\n",
    "> **Note**: Image generation is currently not supported in the `azure-ai-inference` SDK. The example below uses the `openai` SDK with Azure OpenAI's DALL-E 3 model. This section will be updated once image generation is supported in the `azure-ai-inference` SDK.\n",
    "\n",
    "Let's generate a health-themed image using Azure OpenAI's DALL-E-3 model. You'll need:\n",
    "1. A DALL-E-3 model deployment in your Azure OpenAI resource\n",
    "2. The `openai` Python package installed (`pip install openai`)\n",
    "3. Your Azure OpenAI endpoint and API key set in environment variables\n",
    "\n",
    "We'll pass a simple prompt describing a healthy scenario and display the generated image inline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b07b38d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G3D1nzhgApZLlHZckK1I8Lhm8yBzFbDxPmNn84V3pW7g0i0EylLAJQQJ99BBACfhMk5XJ3w3AAAAACOGLwvW\n",
      "https://sarat-m7n4u0uw-swedencentral.services.ai.azure.com/models\n",
      "‚ùå Error generating image: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "print(loaded_config.get(\"AZURE_OPENAI_API_KEY\", \"YOUR-API-KEY\"))\n",
    "print(loaded_config.get(\"AZURE_OPENAI_ENDPOINT\", \"YOUR-AZURE-ENDPOINT\"))\n",
    "\n",
    "def generate_health_image(prompt=\"A simple cartoon of a happy person jogging outdoors\"):\n",
    "    try:\n",
    "        # Initialize Azure OpenAI client\n",
    "        client = AzureOpenAI(\n",
    "            api_version=\"2024-02-01\",\n",
    "            api_key=loaded_config.get(\"AZURE_OPENAI_API_KEY\", \"YOUR-API-KEY\"),\n",
    "            azure_endpoint=loaded_config.get(\"AZURE_OPENAI_ENDPOINT\", \"YOUR-AZURE-ENDPOINT\")\n",
    "        )\n",
    "        \n",
    "        # Generate image\n",
    "        result = client.images.generate(\n",
    "            model=\"dall-e-3\", # your DALL-E 3 deployment name\n",
    "            prompt=prompt,\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "        # Download and display image\n",
    "        image_url = result.data[0].url\n",
    "        image_data = requests.get(image_url).content\n",
    "        \n",
    "        # Save temporarily and display\n",
    "        with open(\"temp_image.png\", \"wb\") as f:\n",
    "            f.write(image_data)\n",
    "        img = Image.open(\"temp_image.png\")\n",
    "        display(img)\n",
    "        \n",
    "        # Clean up\n",
    "        os.remove(\"temp_image.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error generating image:\", e)\n",
    "\n",
    "# Let's try generating a health image\n",
    "generate_health_image(\"A watercolor painting of fresh fruits and vegetables arranged in a heart shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6dece",
   "metadata": {},
   "source": [
    "## 6. Wrap-Up & Next Steps\n",
    "üéâ We've shown how to:\n",
    "- Set up the `AIProjectClient`.\n",
    "- Get **text embeddings** using *text-embedding-3-small*.\n",
    "- Get **image embeddings** using *Cohere-embed-v3-english* on a health-themed (hand X-ray) image.\n",
    "- **Generate** a health-themed image (example code).\n",
    "- Use a **prompt template** to add system context to your embeddings.\n",
    "\n",
    "**Where to go next?**\n",
    "- Explore `azure-ai-evaluation` for evaluating your embeddings.\n",
    "- Use `azure-core-tracing-opentelemetry` for end-to-end telemetry.\n",
    "- Build out a retrieval pipeline to compare similarity of embeddings.\n",
    "\n",
    "Have fun experimenting, and remember: when it comes to your health, always consult a professional!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
